#@ load("@ytt:overlay", "overlay")

#@overlay/match by=overlay.subset({"kind": "ConfigMap"}), expects="1+"
---
metadata:
  #@overlay/match missing_ok=True
  annotations:
    #@overlay/match missing_ok=True
    kapp.k14s.io/versioned: ""

#@overlay/match by=overlay.subset({"kind": "Deployment", "metadata":{"name":"prometheus-alertmanager"}}), expects=1
---
spec:
  template:
    spec:
      volumes: #! force recreate of prometheus-alertmanager if alerting-rules inside prometheus-server configmap change
      #@overlay/append
      - name: prometheus-config-volume
        configMap:
          name: prometheus-server

#@overlay/match by=overlay.subset({"kind": "ConfigMap", "metadata":{"name":"prometheus-server"}}), expects=1
---
data:
  #@overlay/match expects=1
  alerting_rules.yml: |
    groups:
    - name: instances
      rules:
      - alert: Instance_Down
        expr: up == 0
        for: 5m
        annotations:
          title: 'Instance {{ $labels.instance }} / {{ $labels.job }} down'
          description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
        labels:
          severity: warning

    - name: cpu
      rules:
      - alert: CPU_Requests
        expr: (sum(kube_pod_container_resource_requests_cpu_cores) by (node) / sum(kube_node_status_allocatable_cpu_cores) by (node) * 100) > 85
        for: 5m
        annotations:
          title: 'K8s node {{ $labels.instance }} CPU resource requests > 85%'
          description: 'Kubernetes node {{ $labels.instance }} has resource requests on CPU shares over 85% for more than 5 minutes.'
        labels:
          severity: warning
      - alert: CPU_Limits
        expr: (sum(kube_pod_container_resource_limits_cpu_cores) by (node) / sum(kube_node_status_allocatable_cpu_cores) by (node) * 100) > 100
        for: 5m
        annotations:
          title: 'K8s node {{ $labels.instance }} CPU resource limits > 100%'
          description: 'Kubernetes node {{ $labels.instance }} has resource limits on CPU shares over 100% for more than 5 minutes.'
        labels:
          severity: warning
      - alert: CPU_Usage
        expr: ceil(sum (rate (container_cpu_usage_seconds_total{id="/"}[5m])) by (instance) / 2 * 100) > 70
        for: 10m
        annotations:
          title: 'K8s node {{ $labels.instance }} CPU usage > 70%'
          description: 'Kubernetes node {{ $labels.instance }} has a CPU usage of `{{ $value }}%` for more than 10 minutes.'
        labels:
          severity: critical

    - name: memory
      rules:
      - alert: Memory_Requests
        expr: (sum(kube_pod_container_resource_requests_memory_bytes) by (node) / sum(kube_node_status_allocatable_memory_bytes) by (node) * 100) > 75
        for: 5m
        annotations:
          title: 'K8s node {{ $labels.instance }} memory resource requests > 75%'
          description: 'Kubernetes node {{ $labels.instance }} has resource requests on memory over 75% for more than 5 minutes.'
        labels:
          severity: warning
      - alert: Memory_Limits
        expr: (sum(kube_pod_container_resource_limits_memory_bytes) by (node) / sum(kube_node_status_allocatable_memory_bytes) by (node) * 100) > 95
        for: 5m
        annotations:
          title: 'K8s node {{ $labels.instance }} memory resource limits > 95%'
          description: 'Kubernetes node {{ $labels.instance }} has resource limits on memory over 95% for more than 5 minutes.'
        labels:
          severity: warning
      - alert: Memory_Usage
        expr: ceil(sum(container_memory_working_set_bytes{id="/"}) by (instance) / 3000000000 * 100) > 70
        for: 10m
        annotations:
          title: 'K8s node {{ $labels.instance }} memory usage > 70%'
          description: 'Kubernetes node {{ $labels.instance }} has a memory usage of `{{ $value }}%` for more than 10 minutes.'
        labels:
          severity: critical

    - name: disk
      rules:
      - alert: Disk_Usage
        expr: ceil(sum (container_fs_usage_bytes{device=~"^/dev/.*$",id="/"}) by (instance) / sum (container_fs_limit_bytes{device=~"^/dev/.*$",id="/"}) by (instance) * 100) > 70
        for: 10m
        annotations:
          title: 'K8s node {{ $labels.instance }} disk usage > 70%'
          description: 'Kubernetes node {{ $labels.instance }} has a disk usage of `{{ $value }}%` for more than 10 minutes.'
        labels:
          severity: critical

    - name: backman
      rules:
      - alert: Backup_Failure
        expr: sum(rate(backman_scheduler_backup_failures_total[24h])) by (job) > 0
        for: 10m
        annotations:
          title: '{{ $labels.job }}: backup failure'
          description: '{{ $labels.job }} is reporting `{{ $value }}%` failed backups!'
        labels:
          severity: critical
      - alert: Backup_Filesize
        expr: min(backman_backup_filesize_last) by (name, type) < 100000
        for: 10m
        annotations:
          title: 'backman filesize issue detected'
          description: '{{ $labels.type }} backup {{ $labels.name }} is reporting a filesize of only `{{ $value | humanize1024 }}` for its last backup!'
        labels:
          severity: critical

    - name: jcio
      rules:
      - alert: Frontend_Newsfeed_Failure
        expr: rate(jcio_frontend_newsfeed_failures[10m]) > 0
        for: 10m
        annotations:
          title: '{{ $labels.app }}: newsfeed failure'
          description: '{{ $labels.app }} is reporting newsfeed failures!'
        labels:
          severity: warning
      - alert: Stdlib_Newsreader_Failure
        expr: rate(jcio_stdlib_newsreader_feed_failures[10m]) > 0
        for: 10m
        annotations:
          title: '{{ $labels.app }}: newsreader feed failure'
          description: '{{ $labels.app }} is reporting newsreader feed failures!'
        labels:
          severity: warning

    - name: home-info
      rules:
      - alert: Alerting_Error
        expr: rate(homeinfo_dashboard_alerting_errors_total[5m]) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app }}: home-info alerting error'
          description: '{{ $labels.app }} is reporting home-info alerting errors!'
        labels:
          severity: warning
      - alert: Sensor_Error
        expr: rate(homeinfo_dashboard_sensor_errors_total[5m]) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app }}: home-info sensor error'
          description: '{{ $labels.app }} is reporting home-info sensor errors!'
        labels:
          severity: warning

    - name: irvisualizer
      rules:
      - alert: Visualizer_Error
        expr: rate(irvisualizer_errors_total[5m]) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app_service }}: visualizer error'
          description: '{{ $labels.app_service }} is reporting visualizer errors!'
        labels:
          severity: warning

    - name: ircollector
      rules:
      - alert: Client_Login_Failure
        expr: rate(ircollector_api_client_login_error_total[5m]) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app_service }}: API client login failure'
          description: '{{ $labels.app_service }} is reporting API client login failures!'
        labels:
          severity: warning
      - alert: Client_Error
        expr: rate(ircollector_api_client_request_error_total[5m]) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app_service }}: API client error'
          description: '{{ $labels.app_service }} is reporting API client request errors!'
        labels:
          severity: warning
      - alert: Collector_Error
        expr: rate(ircollector_errors_total[5m]) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app_service }}: collector error'
          description: '{{ $labels.app_service }} is reporting collector errors!'
        labels:
          severity: warning

    - name: promtail
      rules:
      - alert: GolangPanic
        expr: avg(promtail_custom_app_panic_total) by (app, app_service, app_service_component) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app }}: Golang panic'
          description: '{{ $labels.app }}/{{ $labels.app_service }} is reporting Golang panics!'
        labels:
          severity: critical
      - alert: Error
        expr: avg(promtail_custom_app_error_total) by (app, app_service, app_service_component) > 0
        for: 5m
        annotations:
          title: '{{ $labels.app }}: error'
          description: '{{ $labels.app }}/{{ $labels.app_service }} is reporting errors.'
        labels:
          severity: info
      - alert: PushFailure
        expr: avg(promtail_custom_repo_mirrorer_failed_push) by (job) > 0
        for: 10m
        annotations:
          title: '{{ $labels.job }}: push failure'
          description: '{{ $labels.job }} is reporting push failures.'
        labels:
          severity: warning
